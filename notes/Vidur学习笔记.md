# Vidur: A Large-Scale Simulation Framework For LLM Inference

## 论文简介

Vidur是一个大型语言模型部署模拟框架，通过实验数据和预测模型相结合，准确模拟不同配置下的模型性能，显著降低成本和提高效率，将原本需要高昂成本的实验过程缩短至数小时，并可将潜在成本降低至实际成本的零头。

Vidur的作用是找到最有效的**部署配置**，而无需进行实际的物理实验：实际的物理实验消耗极大的GPU时间，在时间和资金上都花费极大。

Vidur 结合了实验数据和预测建模来模拟 LLMs 在不同配置下的性能。这种仿真允许评估关键性能指标，如延迟和吞吐量，而无需进行昂贵且耗时的物理试验。

Vidur 的一个关键组件是其配置搜索工具 Vidur-Search，它可以自动探索部署配置。该工具可以有效地确定满足预定义性能标准的最具成本效益的设置。

## Abstract

部署实验的主要耗时在于：需要探索由并行化策略、批处理技术和调度策略等系统旋钮形成的大型配置空间

Vidur——一个大规模，高保真度的，易于扩展的**LLM推理性能仿真框架**。 

Vidur**使用实验性profiling和预测建模结合来模拟LLM算子的性能**，并通过估计延迟和吞吐量等几种感兴趣的指标来评估不同工作负载的端到端推理性能。

模拟误差率仅在9%，但节省了大量的资源和时间。

## 简介

进行推理的成本很高，需要进行部署优化：

- 确定模型并行策略
- 确定模型的调度算法
- 一系列的配置参数，如batch size等，以满足所需的吞吐量和延迟限制
- 生成一些具有代表性的工作负载以进行测试，**这部分的成本很高**

系统地优化具有数百个配置选项的数十个模型的部署是昂贵且不切实际的。

Vidur是模拟器，基于模拟可以使用Vidur-Search来探索最佳配置，使得不用进行实机实验，可以降低时间和成本。

### 模拟的难点

模拟主要针对推理阶段进行模拟，模拟的难点有：

1. 需要在更细的时间进度上保持准确：相较于训练推理所花的时间更少
2. 相较于训练的输入大小几乎确定，推理阶段的输入有较大改变，以及调度策略对预填充和解码阶段的影响，导致迭代延迟有显著的变化
   1. 不可能模拟所有的输入情况，模拟需要分析预测策略
3. 由于推理工作负载的动态和有状态特性，预测中的小错误会导致级联效应

### Vidur的工作

Vidur为了解决上述问题，将LLM进行了抽象，为：绝大多数LLM共享类似的架构，可以拆分为token层次、序列层次和通信运算符

1. 首先接受模型规范，确定需要分析的各种运算符和一组最小的输入大小
2. 构建时间预测模型
3. 预测各类指标，如首次令牌时间（TTFT）、令牌间隔时间（TBT）、延迟、吞吐量，以及集群级指标，例如模型浮点利用率（MFU）和内存利用率。

开发了Vidur-Bench来评估推理心理。跟踪推理中的负载，是一个Benchmark，包括各种工作负载模式、调度器和服务框架，以及流行硬件的分析信息

Vidur-Search通过使用Vidur来模拟各种参数下的效果，确定给定模型、工作负载对的最高吞吐量/成本配置

## 背景

### LLM

LLM基于Transformer的self-attention机制构建

自我注意机制帮助语言模型学习输入序列中不同元素之间的关系，并随后产生输出序列

一个LLM包含两个重要模块：自注意力和多层感知机

### LLM推理效率

LLM推理请求处理由两个不同的阶段组成——预填充和解码

预填充阶段处理整个用户输入提示并生成第一个token，之后，由自回归机制每次生成一个token，直至终结token的出现

解码过程需要访问先前处理的token的key和value激活，以执行注意力操作。为了避免重复计算，现代LLM推理系统将它们存储在KV Cache中。

#### 张量并行TP

张量并行：TP通过将模型权重和KV缓存平均分配给GPU工作线程，在参与的GPU上对每一层进行分片

1. 通过更大的批处理大小提高推理吞吐量
2. 通过在多个GPU上拆分每个运算符来降低推理的延迟

但是张量并行需要频繁的块通信

#### 流程并行PP-Pipeline Parallelism

将不同的流程运行在不同GPU上，每个GPU负责一个运行的stage，输出激活通过send/recv操作跨GPU边界传输

#### Tradeoff

Tradeoff的两端是预填充和解码

- 预填充优先级调度通过生成具有更大批量的调度来实现更高的吞吐量，但会带来更高的延迟成本
- 解码优先级调度器可以实现低延迟，但以较低的吞吐量为代价

#### 模拟的配置空间

LLM推理的最优配置是模型m和trace t的函数，因此配置空间的复杂度是$O(|M|×|T|)$

## 模拟LLM的难点

原先的SOTA-LLM模拟器聚焦训练过程，很少有聚集推理过程的

- 模拟的时间尺度：LLM推理的时间粒度更细，相较于传统的DNN模拟需要更精细的模拟粒度
  - LLM推理是一项对延迟更敏感的任务，迭代时间可以更短
- LLM的每次模拟需求可能变化较大
  - 涉及到多个阶段：prefill和decode
  - 需求的序列长度可能变化很大
  - 在线推理过程中的批处理大小会根据系统负载和工作负载特性而变化
- 在训练任务中，每个iteration都是独立的，不会相互影响。而在推理阶段，request动态到达，如果哪一个batch的模拟误差太大，导致batch模式出现改变，会引起后续的级联错误。

## Vidur

Vidur在副本和集群级别模拟推理栈所有层的行为，包括模型执行和请求调度的各个层

### 关键观点

- 大多数LLM的架构相似
  - 只有激活函数、归一化层、残差连接等地方有不同的选择。
  - 这样可以用一个通用的模型规范来表示多数模型。除此之外，Vidur只需要建模一小部分算子
- LLM的算子可以进行不同分类
  - 一些算子的运行时间取决于这个batch中所有request的上下文长度之和，有的算子的运行时间只取决于当前Iteration的token数量。这样可以对不同类别的算子采取不同方式进行profile
  - 因为在decode阶段attention是memory-bound的计算，因此只使用一个batch的request的KV-Cache总量，就可以精确地建模kernel运行时间
- 并行策略的自动profiling
  - Vidur结合了LLM并行策略的领域知识
  - Vidur结合了关于LLM并行策略的领域知识，使其能够识别在每个设备上执行的计算子集。在profile阶段，我们从model specification中自动识别每个算子的张量切分配置

### 系统概述

Vidur主要有两个阶段。

第一个是**模型装载阶段**：使用model spec生成需要profile的算子集合。

- Vidur profiler收集算子的运行时间特征，并送入runtime estimator。
- 为了减少加入新模型的开销，在profiling过程中收集数据，然后训练一个ml模型用于预测使用的不同参数的算子运行时间。
- 在分析阶段收集最少的数据，然后训练小型机器学习模型，以生成在模拟过程中可能触发这些操作的大范围参数的预测
- 此阶段通过Vidur runtime estimator处理，生成operation-wise的runtime lookup table。

在模型装载完成后，用户可以使用不同的调度策略、并行策略和不同的workload来进行模拟。

- 在Vidur这个event-driven simulator中是一个可插拔的Hierarchical Scheduler，支持多种batching策略和内存管理功能。
- 模拟器提供详细的指标，例如request级别（normalized latency, time-to-first-token, time-between-tokens）和cluster级别（FLOPs utilization, KV-Cache utilization）。

### Profiler：分析器

基于绝大部分LLM相似的特点：基于此特点减少分析的复杂性

#### 算子

所有的算子可以被分为三类：

- Token-level Operator：
  - 一些算子例如linear、avtivation的**操作数维度取决于模型架构**
  - 但它们的**运行时间**只取决于batch中**要处理的token总数**
- Sequence-level Operator：
  - attention不仅取决于当前batch中的token数量，还取决于每个request的上下文长度。
- Communication Operator：all-reduce、all-gather这样的**通信算子**运行时间取决于要传输的数据量，与模型架构无关。

#### Profiling Token-level Operator

token-level算子主要分为两类：矩阵乘法和point-wise（或reduction、nomalization、activation）。

根据模型的规格，生成所有的**张量并行切分配置**，并进行profile。

这样在一个GPU上做profile就可以获得不同并行配置的trace。

#### Profiling Sequence-level Operators

批处理序列级运算符（如attention内核）对批处理中请求的上下文长度很敏感，从而将输入的状态空间扩展到配置文件

将prefill和decode阶段分开，分别做attention的profile，因为不同阶段的计算特性不同

- 在处理prefill阶段的attention时，发现**attention的时间是长度的二次方**

  - 假设有一个batch里有P个序列需要prefill，每个长度为$p_i$，则整个batch prefill attention所用时间与$\sum_{i=1}^p p_i^2$成正比

  - 本文使用一个长度为$\sqrt{\sum_{i=1}^p p_i^2}$的prefill当做一个batch（与上述一个batch中有P个prefill等价），来计算时间

    > 为什么：
    >
    > attention计算时间和prefill sequence的长度l为$O(l^2)$的关系
    >
    > 如果把p1和 p2的两个request 合并成 1个request forward，在attention层面看来需要计算 ($p_1^2$+$p_2^2$)的计算量，就会等价于 $\sqrt{\sum_{i=1}^p p_i^2}$ 长度的prefill request

- decode阶段的attention是**memory-bound的算子**，受到内存中KV-cache的约束

  - 算子的运行时间主要取决于**从KV-Cache中取数据的总量**
  - 而不需要知道一个batch中每个request的上下文长度是多少

当批处理中**不同请求的上下文长度之间存在较大偏差**时，注意力内核可能无法有效地并行化KV缓存获取操作。

然而现有的序列并行attention kernel（PagedAttention v2、FlashDecoding）可以高效地处理这个情况。因此只使用KV-Cache read总量来建模decode是合理的。

#### Profiling Communication Operators

LLM推理中主要使用三种集合通信：

1. all-reduce
2. all-gather：用于张量并行
3. send-recv：用于流水线并行

这些算子的**执行时间是模型无关的**，因此针对不同拓扑在预先就做profile。

### 运行时预估

收集了简单的数据，使用小ML模型对运行时间做估计

在预测训练时间中，简单的多项式估计不能建模CUDA kernel复杂的tile和wave quantization的行为

使用了随机森林进行预估

### 分层调度器

使用了三层分层调度器

1. global：对request进行路由

   1. 用于标准负载平衡策略，如轮询和最小未完成请求

2. replica：对batch和memory进行管理

   1. 包含一个内存计划器，它使用模型规范和并行配置来**计算**KV Cache可用的内存。
   2. 计算得到可用内存后，使用高效内存管理API进行管理
   3. API的使用可以方便引入新的batch管理机制

   > replica：副本管理策略

3. replica stage：处理流水线阶段内的微批调度

## Vidur-Bench

在得到了预测器Vidur后，提出了一系列衡量推理系统性能的指标：

1. 工作负载模式
2. 调度、批处理和路由策略
3. 推理框架

### 数据集

LLM推理对工作负载有类型高度敏感，如在一个请求中的输入和输出token的数量

vLLM为KV-Cache增量分配物理内存，以在GPU上跑更大的batch size，这样当decode token多的时候跑得很好；然而当prompt length比output token数量更多时（如摘要任务），增量内存分配就不那么有用了。

这里的output token是可以进行预测的

### 性能指标

提供了一系列性能指标用于评估推理系统性能

- **Operator-level metrics**：
  - 包括operator的输入尺寸和执行时间
  - 可用于优化重型任务
- **Request-level metrics**
  - 包括scheduling delay, prefill完成时间, timeto-first-token (TTFT), time-between-tokens (TBT)
- **Replica-level metrics**
  - batch size, 每个Iteration处理的token数量, busy和idle时间, 每个replica的内存和计算利用率
- **Hardware-level metrics**
  - 集群的GPU FLOPs和内存利用率

## Vidur-Search

利用仿真结果帮助找到最优的配置

### 仿真约束

包括以下的几个方面：

- 输入：
  - LLM模型
  - 工作workload
  - 可用的GPU
- 约束：
  - SLO（例如TTFT、TBT、QPS）
- 搜索空间：
  - 并行策略（TP vs PP）、并行度、调度策略、调度器参数、batchsize、GPU型号选择
- 优化目标：
  - 最大化QPS per dollar
  - 系统的容量被定义为在**不增加排队延迟**的情况下每秒可以支持的最大查询数

核心是解决**约束下的最优化问题**

- 可能的QPS是无限的
- 对于给定的工作负载，任何系统配置都将具有最大的QPS容量，在该容量下，它可以在不累积请求队列的情况下处理输入请求
- 利用单调性来进行二分，来找到最大的QPS

最佳配置取决于输入工作负载，并且工作负载会随着时间的推移而变化；

## 验证

Evaluation主要回答两个问题：

- Vidur是否可以准确预测不同模型、并行策略、workload的端到端性能。
- Vidur是否可以在给定硬件配置下回答LLM部署的what-if问题

基于VLLM进行了验证

1. 工作负载的变化会极大地改变最佳配置
2. 由于架构细节的变化，即使是尺寸相似的模型也可能具有非常不同的性能特征

## 专有名词

### SKU

在讨论大模型推理加速时，提到的GPU SKU是指特定型号或配置的图形处理单元（Graphics Processing Unit）。SKU是库存保有单位（Stock Keeping Unit）的缩写，在商业中用来表示一个特定的产品变体。对于GPU来说，SKU可以指代不同的方面，比如不同的计算能力、内存大小、功耗、接口类型等。

### QPS

QPS，全称为Queries Per Second，中文译为“每秒查询率”

### Profilling

#### 宏观概念

profiling指的是对模型或用户行为进行分析和评估的过程，以**识别关键特征和性能指标**。具体来说，profiling在LLM中有几个相关的含义：

1. **性能分析工具**：在深度学习和模型优化中，profiling工具用于提供模型**训练或推理过程中**时间和资源消耗的见解，帮助**识别瓶颈和优化资源利用率**。

   1. PyTorch中的`torch.profiler`就是一个性能分析工具，它可以帮助开发者理解模型在执行过程中各个部分的时间和资源消耗情况。
   2. vLLM通过设置环境变量`VLLM_TORCH_PROFILER_DIR`来启用性能追踪。这个环境变量指定了保存追踪数据的目录

   **性能评估**：LLM-Profiler是一个工具，用于评估在线服务引擎的性能，包括速度和吞吐量，适配了多种常见的LLM推理框架。这个工具注重实际在线推理场景下的性能测试，考虑业务延迟要求和符合线上实际请求分布下的系统吞吐量。

2. **内存和计算优化**：在某些研究中，profiling用于**分析不同模块在LLM中的行为**，以便为它们的KV缓存提供不同的优化策略。例如，FastGen算法基于对不同模块行为的分析（即profiling），调整数据存储方式，以提高效率。

profiling在LLM中是一个多面性的概念，既包括技术性能分析，也包括用户行为分析和个性化服务的创建。

#### 在Vidur中的含义

在Vidur框架中，profiling指的是对LLM的**并行策略和操作符的自动性能分析过程**。这个过程包括以下几个关键方面：

1. **模型装载阶段**：在这个阶段，Vidur使用**模型规格**来生成**需要进行性能分析的计算操作符集合**。Vidur的分析器收集这些操作符的运行时间特征，并将其送入运行时估计器。

   **操作符分类**：所有操作符可以分为三类：Token-level Operator、Sequence-level Operator和Communication Operator。

   1. Token-level操作符的运行时间仅取决于批次中要处理的token总数；
   2. Sequence-level操作符的运行时间不仅取决于当前批次中的token数量，还取决于每个请求的上下文长度；
   3. Communication Operator如all-reduce、all-gather等通信操作符的运行时间取决于要传输的数据量，与模型架构无关。

2. **并行策略的自动分析**：不同的模型并行配置有**不同的内存、计算和网络通信特征**。

   1. Vidur结合了关于LLM并行策略的领域知识，使其能够**识别在每个设备上执行的计算子集**。
   2. 在profile阶段，Vidur**从model specification中自动识别每个算子的张量切分配置**。
   3. 因此，Vidur可以使用GPU做最少的profile来模拟各种并行化方案。

3. **运行时估计器**：

   1. 为了减少加入新模型的开销，Vidur在profiling过程中收集数据，然后训练一个机器学习模型用于预测使用的不同参数的算子运行时间。
   2. 这个通过Vidur运行时估计器处理，生成operation-wise的runtime lookup table。

4. **网络（集体操作）分析**：**网络分析不依赖于模型**，因此可以为所有模型使用相同的网络分析数据。

   1. 但是，需要确保网络分析数据适用于所使用的节点配置。

5. **CPU开销分析**：这包括实现开销，如调度时间、采样时间、去标记化等。为了更好的保真度，这些也应该被分析，但它们将模拟器与实现紧密绑定，例如`vLLM`。相关脚本可用，但尚未文档化。

综上所述，在Vidur中，"profiling"是一个涉及对LLM操作符运行时间特征的收集和分析的过程，目的是为了模拟和优化LLM的并行策略和性能

### Profelling

在LLM的推理过程中，prefilling是指在自回归生成之前，对输入提示（prompt）中的令牌（tokens）**计算键值（key-value，简称KV）缓存的过程**。这个过程的目的是为了在生成下一个令牌时避免重复计算，通过预先填充KV缓存来加速推理过程。

具体来说，prefilling阶段会处理整个用户输入的提示，并产生第一个输出令牌。在随后的解码（decode）阶段，会逐个生成输出令牌，直到生成一个特殊的序列结束令牌，此时请求处理完成。在解码过程中，需要访问之前处理过的令牌的键和值激活，以执行注意力操作。为了避免重复计算，现代LLM推理系统会将它们存储在KV-Cache中。

Prefilling的主要作用包括：

1. **减少重复计算**：通过预先计算并存储KV缓存，避免了在生成每个新令牌时重复计算的需要。
2. **提高推理效率**：Prefilling有助于提高LLM推理的速度和效率，尤其是在处理长输入提示时。
3. **优化资源分配**：在处理不同长度的输入提示时，Prefilling可以通过优化计算资源的分配来提高整体的推理性能。

在实际应用中，Prefilling阶段的计算效率较高，因为数据量较大，更容易遇到计算瓶颈。因此，针对Prefilling的优化方向主要是算子合并、简化等，以降低模型计算量。总的来说，Prefilling是LLM推理中一个关键的优化阶段，对于提高模型的响应速度和减少计算资源占用具有重要意义。

### replica

在大型语言模型（LLM）的训练中，“replica”通常指的是模型的一个副本或复制品。在分布式训练或模型部署时，可能会创建模型的多个副本（replicas），以便于并行处理或负载均衡。

#### 训练过程

训练过程中，每个replica都**有完整的参数，但是对不同的数据集**进行训练。

参数可以独立更新，或者在某些情况下，通过同步机制来保持一致性。这样做可以提高训练效率和模型服务的可用性。

确保各个副本（replicas）之间的一致性以及最终训练得到统一的模型，主要依赖于以下几个关键技术和策略：

1. **数据并行性**：在数据并行训练中，数据集被分割成多个分片，每个分片分配给不同的设备。每个设备保存模型副本的完整副本，并在分配的数据集分片上进行训练。反向传播后，模型的梯度通过All-Reduce操作全部减少，以保持不同设备上的模型参数同步。
2. **全分片数据并行（FSDP）技术**：FSDP技术在数据并行worker中统一分片模型参数并训练数据，其中每个微批数据的计算均针对每个GPU worker进行本地计算。FSDP通过操作重排序和参数预取最大限度地减少气泡，以通过计算积极地重叠通信。
3. **同步训练算法**：同步训练算法如All-Reduce是确保各计算节点间梯度同步的重要手段。所有节点计算出本地梯度后，通过高效的通信协议（例如Ring All-Reduce）汇总所有节点的梯度信息，然后统一更新模型参数。
4. **参数服务器架构**：Parameter Server作为中心化的存储和协调器，负责维护和更新模型参数。各个计算节点异步地从参数服务器读取参数，计算局部梯度，再将梯度发送回参数服务器进行更新。
5. **异步训练与优化策略**：异步训练允许不同节点根据自己的进度更新全局模型，但可能导致不稳定的收敛性和一致性问题。实践中，研究者尝试通过控制更新频率、优化通信策略以及使用延迟补偿等方法平衡效率与稳定性。
6. **超大规模LLM中的通信开销与梯度一致性解决方案**：为降低通信开销，可采取梯度压缩、稀疏通信、选择性通信等策略。同时，为了保证梯度一致性，还引入了诸如同步屏障、动态调整学习率等算法和技术。
7. **零冗余优化器（ZeRO）**：ZeRO通过在所有GPU上完全分片模型状态来优化内存冗余。在训练过程中，每个GPU进行独立的前向和后向传播来计算梯度，然后使用ReduceScatter操作在数据并行组内的所有GPU之间同步梯度。每个GPU负责更新特定部分的模型参数。随后，更新后的模型参数片段从其他GPU上收集，使用AllGather操作，确保所有GPU都有最新的模型参数。

通过上述技术和策略，分布式训练过程中的各个模型副本能够保持一致性，并最终训练得到一个统一的模型。

#### 推理过程

replica在推理过程中起到了提高效率、可用性、吞吐量和资源共享的作用。

1. **分布式推理**：在分布式计算环境中，"replica"指的是模型的不同副本
   1. 它们可以**分布在不同的计算节点上。这样可以并行处理多个推理请求**，提高模型的响应速度和吞吐量
   2. 每个副本可以独立地处理一部分任务，从而实现负载均衡和提高效率。
2. **模型可用性**：在推理过程中，"replica"可以增加模型的**可用性和容错能力**。如果一个副本出现问题或者需要维护，其他副本可以继续提供服务，确保不间断的推理能力。
3. **提高吞吐量**：在某些情况下，"replica"可以类似于数据库中的分区（partition），每个分区存储一部分数据。这样可以提高消息写入的吞吐量，并且可以提高消费者消费消息的并发量。在LLM推理中，这意味着可以同时处理更多的请求，提高整体的处理能力。
4. **资源共享**：有些项目，如Petals，通过分布式计算的方式运行大型语言模型，采用类似BitTorrent的方式，将模型的不同部分分布在多个用户的设备上，实现高效的推理和微调。这种方式依赖于社区用户共享GPU资源，用户可以贡献自己的GPU来增加计算能力，这里的"replica"就是指分布在不同设备上的模型副本。
5. **API接口和模型管理**：在API接口中，"replica"的数量可以作为参数之一，用于指定模型副本的数量。这有助于在不同的负载和需求下调整资源分配，优化性能。

### 流水线并行

流水线并行（Pipeline Parallelism）是一种在分布式计算环境中实现模型并行的技术，主要用于深度学习领域，尤其是在处理大规模神经网络模型时。以下是流水线并行的关键特点和应用：

1. **定义与原理**： 流水线并行通过将模型的不同部分（如神经网络的不同层）**分配到不同的计算节点上**，使得这些部分可以并行处理，从而提高计算效率和资源利用率。这种技术允许**不同硬件资源上的计算和通信重叠**，加速训练过程。
2. **优势**：
   - **提高计算效率**：通过**并行处理多个子任务**，流水线并行计算能够显著缩短任务的总体执行时间。
   - **优化资源利用**：合理分配计算资源，使得每个处理单元都能得到充分利用，避免资源浪费。
   - **增强系统可扩展性**：随着处理单元数量的增加，流水线并行计算的性能可以线性或超线性增长。
   - **降低系统延迟**：通过并行处理减少单个任务的等待时间，提高系统的响应速度。
3. **应用场景**： 流水线并行特别适用于层数较多、层间依赖关系较弱的神经网络模型，如Transformer、BERT等。这些模型通常具有较大的参数量和计算量，单机训练难以承受，而流水线并行则能有效缓解这一问题。
4. **关键技术**：
   - **微批次（Micro-Batch）**：为了充分利用计算资源，流水线并行通常会将大批量数据切分成多个微批次，每个微批次独立在流水线中流动。
   - **通信优化**：减少阶段间的通信开销是提升流水线并行效率的关键。常见的优化方法包括重叠计算和通信、使用高效的通信协议等。
5. **实践建议**：
   - **合理选择切分点**：根据模型的结构和计算需求，合理选择切分点以平衡不同阶段的计算负载。
   - **优化通信策略**：采用重叠计算与通信的策略，减少通信开销对训练速度的影响。
   - **使用高效框架**：利用支持流水线并行的深度学习框架，如MindSpore、PyTorch等，简化并行训练的实现过程。

流水线并行是一种强大的技术，它通过在多个计算节点上并行处理模型的不同部分，显著提高了大规模神经网络模型的训练效率和资源利用率。

### 张量并行

张量并行（Tensor Parallelism，TP）是一种模型并行技术，它通过**将神经网络中的单个层的内部计算分割到多个处理单元（如GPU）上**执行，从而实现并行计算。以下是张量并行策略的一些关键点：

1. **细粒度并行**： 张量并行是一种更细粒度的模型并行方法，它适用于具有大量参数的大规模模型。通过将矩阵乘法等计算操作的矩阵按行或按列切分，然后在不同设备上并行执行部分计算，最后通过集合通信操作合并结果。
2. **适用场景**： 张量并行特别适合于参数众多的大规模模型，如Transformer模型。它通过将模型的单个层（例如一个大型的矩阵乘法操作）的计算分割成更小的部分，并在多个计算设备上并行执行，从而减少单个设备上的内存需求，同时加速计算过程。
3. **实现方式**： 张量并行可以通过多种方式实现，包括MatMul并行、Transformer并行、Embedding并行、Cross Entropy Loss并行等。序列并行（Sequence Parallel，SP）也是张量并行的一种变体，它在序列维度上对nn.LayerNorm或RMSNorm进行分割，以进一步节省训练过程中的激活内存。
4. **通信优化**： 在张量并行中，为了确保计算的正确性，需要在不同设备间进行通信。例如，在计算softmax时，需要进行All Reduce (Max)操作以获取全局最大值，防止溢出；在计算exp sum时，也需要All Reduce (Max)操作以获取全局的和。

### 模型的交叉验证

交叉验证（Cross-Validation）是一种统计分析方法，用于评估并提高模型的预测性能，特别是在机器学习中。

交叉验证最常见的形式是 k 折交叉验证（k-fold cross-validation），其中数据集被随机分成 k 个相等的子集，模型在 k-1 个子集上训练并在剩下的一个子集上验证，这个过程重复 k 次，每个子集都作为验证集使用一次。最终的性能评估是 k 次验证结果的平均值。这种方法有助于确保模型评估的一致性和可靠性。

主要有以下几个作用：

1. **评估模型泛化能力**：

   交叉验证通过将数据集分成多个子集，确保模型在不同的数据子集上进行训练和验证，从而评估模型对未见数据的泛化能力。

2. **减少过拟合风险**：

   通过在不同的数据子集上重复训练和验证过程，交叉验证有助于检测模型是否过拟合于特定的训练数据。

3. **提高模型稳定性**：

   交叉验证可以确保模型在不同的数据分布下保持稳定性，因为它要求模型在多个训练集上都能表现良好。

4. **优化超参数**：

   在超参数优化过程中，交叉验证用于评估不同超参数设置的性能，帮助选择最佳的超参数组合。

5. **更有效的数据利用**：

   特别是当数据集较小时，交叉验证可以更有效地利用有限的数据，因为它允许每个数据点在训练和验证过程中被多次使用。

6. **减少结果的方差**：

   单一的训练/测试数据分割可能会由于随机性而导致模型评估结果的高方差。交叉验证通过多次分割减少了这种方差，提供了更可靠的性能估计。

7. **适用于小数据集**：

   对于小数据集，传统的训练/测试分割可能会浪费大量数据。交叉验证允许所有数据点都参与到训练和验证中，提高数据的使用效率。

8. **模型选择**：

   在比较不同模型的性能时，交叉验证提供了一个公平的比较平台，因为它确保了每个模型都是在相同的数据集上进行评估的。

# The Effect of Scheduling and Preemption on the Efficiency of LLM Inference Serving

## 论文简介

是Vidur的后续工作，主要研究了在LLM推理服务中，**调度和抢占**对效率的影响。

发现了请求长度和抢占之间的关系

核心就是一个CSP问题，围绕CSP问题进行论文的流程

## Abstract

LLM的旺盛需求对推理系统提出了要求，在部署和开发过程中均是。

但是当下并没有对调度器优劣的全面分析。原先在GPU上进行实验测试的成本是很高的。

提出了InferMax，使用**推理成本**来对不同的调度器进行比较

## 简介

和训练不同，推理是一个持续性的高花费操作，对经济和能源都有较高需求

- 已有的ChatGpt揭示了运行时的高花费
- 新的模型如O1，多轮思考过程进一步加大了花费

调度有着重要的作用：

- 错误调度会使得吞吐率降低6倍
- 一些调度的最优算法可能是反直觉的
- 抢占请求可以减少GPU成本高达30%，与避免抢占相比。
- 请求长度和抢占之间的相关性很难发现。

InferMax是一个推理框架，比较不同的调度器和调度策略，并与最优进行比较

VIDUR专注于模拟和搜索用于部署的最佳硬件模型调度器配置，但缺乏对进一步提高性能的机会的深入分析和探索，这为开发留下了空白。

提出了图个**作为约束满足问题（CSP）**的最优调度器，以建立性能的上限：**将CSP问题的解决方案作为最优解**

以约束的形式强制特定的调度策略，并优化延迟、吞吐量或任何可以表示为公式的目标。

## 背景

介绍了如何处理LLM请求

### 处理单个LLM请求

prompt：用户输入的文本

prefill：处理输入的token，产生下一个token

decode：紧随prefill，每一个根据上一步产生的token产生一个最新的token

1. 将输入的token进行embedding为向量
2. 将向量移动到显存
3. 在输入向量和**模型权重**之间进行操作，如矩阵乘法和注意力操作
4. 在**所有的模型层**上重复之星3，产生和输入向量大小匹配的输出向量
5. 将最后一层的输出**送到sampling组件**以产生下一个token

大多数的LLM都依赖于Transformer架构：

- 通过使用每个token查询（Q）以及先前token的键（K）和值（V）向量来计算每个token的注意力输出
- 减少内存搬运，KVcache存储在GPU中

在这种结构中：

- prefill tokens缺少KV的先前token，使其处理受到**计算限制**
- 相比之下，随着输出长度的增长，decode tokens有一组不断扩展的KV可供读取，导致解码步骤变得越来越**内存受限**

### 处理多个LLM请求

推理系统依赖于调度器在每一步进行**批处理**：优先考虑资源利用率，特别是与token限制相对应的最大KV缓存大小M

每一步的可处理最大token数C为：
$$
C = \frac{memory_{gpu\_available} - memory_{model\_size}}{memory_{KV\_size\_per\_token}}
$$
其中GPU的可用内存通常按照90%进行计算

为了提高效率，使用了数据库和OS的技术，两种标准方法为**连续批处理**和**分页注意力机制**（TODO：需要阅读这两篇论文）

- 连续批处理在资源可用时**逐步调度新请求**，避免空闲等待时间
- 分页注意力通过**管理页面段中的KV-Cache**，而不是为每个请求都保留大量分配，来提高内存利用率

为了进一步管理调度，定义了两个指标：

1. 批大小：每个批中的请求个数
2. 运行大小：持有KV-Cache的请求数量

驱逐：当正在运行的请求的KV总数达到缓存预算M时，系统可能需要抢占一些请求，清除它们的KV缓存。

一旦被驱逐，请求生成的token将附加到prompt中。将再次处理这些令牌称为**再填充阶段**refill。

### 相关工作

#### LLM推理系统

如vllm（TODO：尽快读这篇文章）

#### LLM推理仿真

Vidur。VIDUR主要强调模拟和搜索，缺乏深入的分析或结构化的方法来设计改进的调度器

#### 输出尺寸大小预测

估计或排序请求的输出大小以提高调度效率

## InferMax

工作流程：

1. 给定初始配置①，内置或自定义调度器生成调度②，定义每个批中处理的请求集。
2. 这些调度器可以被分派到推理系统③以执行和衡量性能④
3. 使用VIDUR的结果⑥进行预测：根据处理的令牌数量和访问的KV缓存⑤**预测批处理执行时间**
4. 作为替代方案，理论硬件边界，如在LLMVIEWER中使用的边界，通过roofline models应用和可视化⑦

使用Vidur的原因：

1. **成本效益**：实际在GPU上测试每个调度器配置可能会非常昂贵。VIDUR允许研究人员和开发者在不实际运行GPU的情况下模拟调度器的行为，从而节省了大量的硬件资源和成本。
2. **性能预测**：VIDUR提供了执行时间数据，这些数据可以用于预测不同模型和GPU配置下GPU操作的时间。这使得研究人员可以在不实际执行推理的情况下，评估不同调度器的性能。
3. **优化调度器开发**：通过模拟，VIDUR可以帮助识别哪些调度策略可能更有效，从而指导更有针对性的开发工作，减少无效尝试和错误，节省开发资源。
4. **探索性能极限**：VIDUR可以帮助研究人员理解在给定硬件和模型配置下，调度器性能的理论上限。这对于设计能够接近或达到这些上限的高效调度器至关重要。
5. **灵活性和可扩展性**：VIDUR作为一个模拟框架，可以轻松地扩展和适应新的硬件、模型和调度策略，这使得研究人员可以在一个统一的平台上测试和比较不同配置。
6. **深入分析**：VIDUR不仅提供了模拟功能，还允许进行深入分析，探索性能提升的机会，挑战现有的假设，并发现新的调度优化方法。
7. **支持CSP解决方案**：VIDUR的结果可以用来初始化INFERMAX中的约束满足问题（CSP），以寻找最优调度策略。这种结合模拟和优化的方法可以更有效地找到性能最佳的调度策略。

### 调度器的统一算法

开发了一种跨各种推理系统的调度器的**统一算法**：在GetNextBatch函数的实现方面有所不同

#### 符号

- KV-Cache最大大小M
- 每批的最大token数C
- 每批的最大**预填充**token数P
- 等待request队列$R_w$
- 运行request队列$R_r$
- 最大运行大小$R_{max}$：持有KV-Cache的请求数量

批大小：每个批中的请求个数

运行大小：持有KV-Cache的请求数量

#### 算法

输入：

- KV-Cache最大大小M
- 每批的最大token数C
- 每批的最大**预填充**token数P
- 最大运行大小$R_{max}$：持有KV-Cache的请求数量

运行：

1. 清空等待request队列$R_w$
2. 清空运行request队列$R_r$
3. 进行循环：
   1. $R_w$添加新产生的请求
   2. 根据GetNextBatch函数获取新的处理批B
   3. 处理B
      1. 将输出发送给用户
      2. 从B中移除已经处理的请求
      3. 得到$B' = Process(B)$，指还未处理的请求
   4. 将$R_r$更新为$R_r \cup B'$

GetNextBatch函数：

- 参数：

  - 等待队列
  - 运行队列
  - KV-Cache最大大小M
  - 每批的最大token数C
  - 每批的最大**预填充**token数P，即可处理的需要进行prefill的token数
  - 最大运行大小$R_{max}$：持有KV-Cache的请求数量

- 返回：

  - 一个批请求B

- 运行：

  1. 将批请求B进行初始化为空
  2. 对$R_w \cup R_r$中每一个请求组$G \in R_w \cup R_r$:
     1. 对G中的每一个请求cand：cand代表candidate
     2. 如果混合批处理禁用：如果cand和B中的请求处在不同的阶段prefill/decode，则跳过cand的处理
     3. 根据CanAllocate检查cand是否能添加到B中
        1. 如果由于**M不够而不能添加**，调度器可以选择驱逐其他request以处理cand还是处理下一个cand
        2. 如果由于其他原因不能添加，不会发生驱逐，处理下一个请求
        3. 被驱逐的请求会被从$R_r$中移除，添加到$R_w$中
     4. 如果成功添加，r会添加到B中，从原先的队列中移除

- CanAllocate的检查：根据一系列因素检查是否可以添加新的批：需要对**批的每一个请求**进行检查

  1. 批B在添加cand后，所有的request的token不超过最大token数C

     每批最多可以处理$C- \sum_{r \in B} r_{tokens}$个token，添加后的token不能超过

  2. 如果cand处于prefill阶段，那么还受到了最多prefill-token数P的限制

     每批进行prefill的token最多可以处理$P- \sum_{r \in B} r_{tokens}$个，添加后的prefill-token不能超过

     如果时**块状预填充**chunked-prefill，cand中的token数可以更小，为$cand_{tokens} = \min(cand_{toens}, P - \sum_{r \in B} r_{tokens})$

  3. 添加后对KV-Cache的需求不超过最大限制M

  4. 处理的最多request数M的限制：添加request后不超过

#### VLLM的处理

对于VLLM而言：

- 队列被分为两组（每组有多个）$R_r, R_w$，其中$R_w$中的**prefill-request具有优先级**
- 混合批处理和块状预填充是禁用的
- 在处理cand时，如果M不足，不驱逐，处理下一个cand
- 对于$R_r$中的请求，会

### 批处理时间的成本模型

建立了一个模型预测批处理时间

时间包括：

1. non-attention的操作，如MLP和Activation
2. decode阶段的attention，受从cache中读取的KV数决定
   1. memory-bound，受KV读取速度限制
3. prefill阶段的attention，与token数的**平方**成正比
   1. compute-bound，表现出二次复杂性
4. 偏差项：
   1. 捕获了额外的固定成本，例如加载模型权重和启动内核
   2. 提高GPU带宽和FLOP，或减小模型和KV大小，可以降低这些模型系数和偏差。

从论文中的图看：

- 1与token数成正比，线性相关性好
- 2与KV数成正比，线性相关性好
- 3与token数的平方成正比，线性相关性好

未来的工作包括在硬件、模型类型和系统之间建立一个**统一的成本模型**，并使用强化学习（RL）或贝叶斯优化来完善这个成本模型

在最近的推理系统中，与**调度和KV缓存管理相关的CPU开销**可能会与GPU处理在很大程度上**重叠**，因为批处理是**异步调度**的，GPU计算是关键路径

### 基于CSP的最优调度

设计了一个约束-满足问题（CSP问题）来确定最优调度

可以通过调整约束问题的目标来寻找目标调度。将CSP调度作为了寻找最优调度的一种方式

假设了**request的输出大小是预先知道的**：使用了输出大小估计技术

#### 符号

- i：第i个request-$r_i$，从1开始
- j：第j个batch-$B_j$，从1开始
  - 使用j=0作为**虚拟索引**，代表初始的系统状态
- $I_i$：$r_i$的输入大小，是prompt的token数
- $O_i$：$r_i$的输出大小
- L：指标变量

#### 变量

- $I_{i, j}$：在处理$B_j$后输入大小

  - 在处理$B_j$的过程中可能会发生驱逐，导致输入大小发生改变

  - $I_{i, 0} = I_{i}$

- $m_{i, j}$：$r_i$的在处理$B_j$后的内存

  - $m_{i, 0} = 0$
  - $r_i$的最大内存使用量可以达到$I_i+O_i−1$，因为最后生成的token不需被放入cache以供之后的token生成

- $d_{i, j} = L({m_{i, j-1} \ge I_{i, j-1}}) \in {0, 1}$：代表$r_i$是否在$B_j$之前已经处理了所有的token，揭示**是否处于decode阶段**

- $g_{i, j}$：代表在$B_j$阶段，$r_i$产生的token数

- $e_{i, j}$：代表在$B_j$阶段，$r_i$是否被驱逐

- $c_{i, j}$：$r_i$在$B_j$的token数

以下是一些指标变量：

- $c z_{i, j} = L({c_{i, j} \ge 0})  = L(r_i \in B_j)$：表示请求i在第j个批次中至少处理了一个令牌，即$r_i$属于批次$B_j$
- $m z_{i, j} = L(m_{i, j} \ge 0)$：表示请求i在第j个批次中**有内存使用**
- $d z_{i, j} = L(d_{i, j} \and g_{i, j})$：表示请求i在解码阶段生成了一个令牌

#### 约束

终止约束：$r_i$必须产生$O_i$个token

- 这个约束确保每个请求$r_i$必须生成其预期的输出令牌数量$O_i$
- 其中$g_{i, j}$是一个二进制变量，表示请求$r_i$在批次$B_j$中是否生成了一个令牌。

$$
\sum_{j} g_{i, j} = O_i
$$

输入大小非递减：+1代表了token产生了但还没有处理，**至少**产生一个新token
$$
I_{i, j} = 
\begin{cases}
m_{i, j-1} + 1 
\quad , e_{i, j} = d_{i, j} = 1
\\
I_{i, j-1}
\quad , else
\end{cases}
$$
内存管理：

- 当请求被抢占（evicted）时，内存使用量设置为0；否则，内存使用量根据当前处理的令牌数量$c_{i, j}$增加
- 如果请求没有被抢占且处于预填充和解码阶段，则内存使用量不能超过输入大小的限制。
- 这个约束确保内存使用的合理性，避免超出可用内存。

> $c_{i, j}$是token数，这里参与内存问题是有些奇怪

$$
m_{i, j} = 
\begin{cases}
0
\quad , e_{i, j} = 1 \\

m_{i, j-1} + c_{i, j}
\quad , e_{i, j} = 0 \\

\le I_{i, j-1}
\quad , e_{i, j} = d_{i, j} 0 \\
\end{cases}
$$

c在驱除和decode的控制：

- 这个约束控制在抢占和解码阶段请求的处理数量。
- 如果请求被抢占，则在该批次中处理的令牌数量为0；
- 如果请求**处于decode阶段，则处理的token数量最多为1**。
- 这个约束确保在特定阶段的请求处理符合逻辑，避免因抢占或解码导致的错误处理。

$$
c_{i, j} = 
\begin{cases}
0 
\quad , e_{i, j} = 1 \\

\le 1
\quad , d_{i, j} = 1
\end{cases}
$$

批约束：确保每个批的全局约束

- 混合批处理和分块预填充都是启用的
- 第一个约束确保每个批次中处理的令牌总数不超过最大令牌数 C。
- 第二个约束确保在批次中处理的预填充令牌数量不超过最大预填充令牌数P
- 第三个约束确保所有请求的内存使用总和不超过GPU的最大内存M
- 第四个约束确保活动请求的最大数量不超过设定的最大运行大小$R_{max}$

$$
\sum_{i} c_{i, j} \le C \\
\sum_{i} c_{i, j} - d g{i, j} \le P \\
\sum_{i} m_{i, j} \le M \\
\sum_{i} m z_{i, j} \le R_{max}
$$

#### 目标

目标为**最小化总延迟**

也可以使用其他的替代目标，如request层级的目标

- 产生第一个token的时间（TTFT）：$\min_j (Acc_j g_{i, j} - T_i)$
- token输出平均时间（TPOT）

#### 在线需求的支持

使用时间模拟实际的在线需求：对每个$r_i$，设置到达时间$T_i$

使用$Acc_{j}$跟踪累计批处理时间

如果$Acc_{j} \lt T_i$，则设置$c_{i,j} = m_{i,j} = 0$

## 分析

### 实验设置

使用vllm和Sarathi作为baseline

使用离线设置，在调度开始之前，所有请求都可用，这相当于在线环境的快照

### 高竞争场景

#### 驱除研究

VLLM通过并行优先处理prefill请求和批处理decode来展示最低的延迟和最高的TPS，从而产生大的批大小。除非高O值导致频繁驱逐

随着O的增加，因为每个request都在竞争保留其KV-Cache

#### TPS

TPS：吞吐率

随着批大小B的增大，TPS增加

随着I的增大，B减小，prefill阶段的花销增加

TPS降低是因为小的O值使prefill占主导地位，而大的O值则使后续的decode批更关键，因为decode成本与读取的token数或KVs的数量呈线性关系

#### 驱逐和批大小

M限制了预填充或解码阶段的**最大运行请求数**，而每批新运行请求的数量则受到prefill的token数P的限制

对I和O的本质：

- I表示保留的即时内存
- O确定处理大约$\Omega(n)$批后的**峰值内存使用量**

#### 内存使用/KV-Cache使用

和对request的处理相关，如果不进行驱逐，那么request会持有内存，造成较高的内存使用

#### 评论

高TPOT主要来自驱逐，批量大小和KV负载是次要因素

驱逐随B和O的增长而增长

### 避免驱逐是好方法吗

结论：

- 驱逐小request可以提升性能
- 大请求、大输出的驱逐会降低性能

对于无驱逐：

- 吞吐率表现更好，TPS有一定上升
- TTT更高：由于需要等待正在运行的请求完成并释放其KV

### 提升M好吗

SOTA-GPU的表现是增加显存大小，真的好吗：单独增加M不一定管用，显存通信带宽也很重要

- 当M较小时，无驱逐的TPS较低，因为需要长时间等待显存释放
- 对于足够大的M，即使对于较小的I，驱逐也会最小化，导致每个decode-batch快速将M/I个 KV添加到内存中，而对于较小的M，这反而会导致频繁的驱逐，并增加TPOT。

### 驱除一定坏吗

在M较小的情况下，驱逐可以降低延迟：通常更快地开始生成带有prefill的token，然后在以后的批处理中驱逐请求，而不是等待释放足够的内存。

尽早处理较小的request对提升整体的吞吐率有明显作用

驱逐大request由于较高的refill成本会导致成本升高

### $R_{max}$应该多大

限制运行request的数量以避免驱逐长请求和处理大量KV高要求的批处理至关重要

M起到了限制总处理token数的作用

- 对于较小的I/O，R应较大
- 对于较大的I/O，R应较小

## 总结

强调对驱逐和内存需求的影响进行分析可以带来显著的性能提升

# 代码阅读

Vidur对代码进行了开源

InferMax未对代码进行开源

## 开始

### 环境准备

使用conda/mamba配置本地的包环境，没有集中安装到conda的env_list中

```shell
mamba env create -p ./env -f ./environment.yml
# 选择本地的环境进行代码运行
conda activate ./env
```

### 运行

整个vidur被封装为了一个python包，以包为整体对象进行运行

```shell
# 不使用参数
python -m vidur.main

# 使用完整参数
python -m vidur.main  \
--replica_config_device a100 \
--replica_config_model_name meta-llama/Llama-2-7b-hf  \
--cluster_config_num_replicas 1 \
--replica_config_tensor_parallel_size 1 \
--replica_config_num_pipeline_stages 1 \
--request_generator_config_type synthetic \
--length_generator_config_type trace \
--interval_generator_config_type static \
--[trace|zipf|uniform|fixed]_request_length_generator_config_max_tokens 4096 \
--trace_request_length_generator_config_trace_file ./data/processed_traces/arxiv_summarization_stats_llama2_tokenizer_filtered_v2.csv \
--synthetic_request_generator_config_num_requests 128  \
--replica_scheduler_config_type vllm  \
--[vllm|lightllm|orca|faster_transformer|sarathi]_scheduler_config_batch_size_cap 256  \
--[vllm|lightllm]_scheduler_config_max_tokens_in_batch 4096
```

#### 参数含义

1. **Number of replicas**：这个参数指的是副本的数量，也就是你希望运行的服务实例的数量。在Vidur中，这可能涉及到模型或服务的并行部署，以提供高可用性和负载均衡。
2. **TTFT (Time-to-First-Token)**：这是从请求到达系统到生成第一个输出标记（token）的时间。它是衡量系统响应速度的一个重要指标。
3. **TBT (Time-Between-Tokens)**：这是用户观察到的标记之间的延迟，即系统生成连续输出标记之间的时间间隔。
4. **TP (Tensor Parallelism)**：指的是张量并行，是一种模型并行技术，用于在多个GPU上分布模型的不同部分以加速训练或推理。
5. **PP (Pipeline Parallelism)**：指的是流水线并行，是一种并行技术，用于将模型的不同层分配到不同的GPU上，以提高模型的吞吐量。
6. **Request E2E Time**：这是请求的端到端时间，即从请求到达系统到请求完全处理完成的时间。
7. **Batch Size**：这是每个批次处理的请求数量。较大的批次大小通常意味着更高的吞吐量。
8. **Prefill Tokens**：这是请求中用于填充的标记数量，是模型推理过程中的一部分。
9. **Decode Tokens**：这是请求中用于解码的标记数量，也是模型推理过程中的一部分。
10. **P:D Ratio (Prefill to Decode Ratio)**：这是填充标记与解码标记的比例，它可以帮助理解工作负载的特性。

## 层次结构

阅读代码可以得到如下的层次结构：

- Cluster：代表了对最顶层训练集群的抽象，包含了模型、设备、请求。是最顶层的抽象

  - replica：分布式推理过程会有多个推理实体，可以并行地响应request，提高响应速度和吞吐量

    - 每个replica中的模型都是相同的，共享一套参数

  - schedule：调度策略

    - global：对外部（也即顶层）的request进行调度。经典的调度算法与round-robin

    - replica：在每个replica的层次上进行调度，包括batching策略和内存管理策略

      - 通过模型的信息和并行策略计算得到可用于KV-cache的内存
      - 在计算得到内存信息后，通过batching策略的API接口进行调度，如vllm

      > 在需要高吞吐量和高内存效率的情况下，直接运行LLM可能会面临性能瓶颈
      >
      > 相应的batching策略通过**优化内存管理和计算效率**，使得LLM在这些场景下的性能得到显著提升

    - replica_stage：最底层的调度策略：在流水线层级的微批调度

### 策略类

策略类都继承了同一个类`BasePolyConfig`

### Token

#### Batch

在 `Batch` 类中提到的三种token（`num_tokens`、`num_prefill_tokens`、`num_decode_tokens`）分别有不同的含义，并且它们在处理请求时扮演不同的角色。以下是每个token的具体含义和它们的区别：

1. **`num_tokens`**：
   - `num_tokens` 是一个列表，其中每个元素代表对应请求中的token数量。它是每个请求需要处理的token总数的简单计数。
   - 例如，如果一个请求包含句子“Hello, how are you?”，并且模型以空格分隔单词，那么这个请求的 `num_tokens` 就是4。
2. **`num_prefill_tokens`**：
   - `num_prefill_tokens` 表示这批请求中所有**预填充**（prefill）请求的token总数。
     - 预填充请求是指那些**已经预先处理过**一部分token的请求
     - 例如，在连续的对话系统中，后续请求可能依赖于之前请求的部分结果。
   - `num_prefill_tokens` 的计算方式是：对于每个请求，如果请求的 `is_prefill_complete` 属性为 `False`，则将该请求的token数量加到总数中；如果为 `True`，则不添加。
   - 例如，在处理一个长对话时，如果前一个请求已经处理了对话的前半部分，那么下一个请求可能只需要处理后半部分，预填充的token就是已经处理过的部分。
3. **`num_decode_tokens`**：
   - `num_decode_tokens` 是指这批请求中**需要解码的token**总数。它通常是 `total_num_tokens`（所有请求的token总数）减去 `num_prefill_tokens`（预填充的token数）。
   - `num_decode_tokens` 表示实际需要模型进行解码操作的token数量，也就是模型需要新生成的token数量。
   - 例如，如果一个请求的 `num_tokens` 是10，而 `num_prefill_tokens` 是2（假设前两个token已经预填充），那么 `num_decode_tokens` 就是8。

#### Request

预填充token数量（`num_prefill_tokens`）、解码token数量（`num_decode_tokens`）和已处理token数量（`num_processed_tokens`）之间的关系和转化如下：

1. **预填充token数量（`num_prefill_tokens`）**：
   - 这是指在推理过程的预填充阶段，模型并行处理输入的tokens。
   - 这些tokens通常是请求的初始部分，模型会一次性计算这些tokens的输出。
   - 在预填充阶段，模型会生成第一个输出token，这个阶段主要是计算瓶颈。
2. **解码token数量（`num_decode_tokens`）**：
   - 这是指在推理过程的解码阶段，模型逐个生成下一个token。
   - 这些tokens是在预填充阶段之后需要生成的，这个阶段使用了之前计算好的KV Cache来加速推理
   - 同时当前前向过程中计算的新的key和value也被缓存起来，以便下一轮使用。
3. **已处理token数量（`num_processed_tokens`）**：
   - 这是指在推理过程中，模型已经处理的token总数。
   - 它包括预填充阶段处理的tokens和解码阶段已经生成的tokens。
   - `num_processed_tokens`随着模型的推理逐步增加，直到达到请求的总token数（`num_prefill_tokens + num_decode_tokens`）。

**关系和转化**：

- `num_processed_tokens`初始值通常为0，在预填充阶段，`num_processed_tokens`会增加直到等于`num_prefill_tokens`。此时，预填充阶段完成，模型开始解码阶段。
- 在解码阶段，每生成一个新的token，`num_processed_tokens`就会增加1。当`num_processed_tokens`等于`num_prefill_tokens + num_decode_tokens`时，表示整个请求已经处理完成。
- `num_processed_tokens`的变化反映了模型处理请求的进度，从预填充阶段到解码阶段，直到整个请求完成。

这种关系和转化是LLM推理过程中的关键，它们决定了模型的执行时间和性能指标，如Time To First Token (TTFT)和Time Per Output Token (TPOT)。通过优化这些阶段，可以显著提高LLM的推理效率和响应速度。

## 模拟

有多种多样的模拟

### 计算结构

使用了MLP和Attention进行简单的推理建模。

在大型语言模型（LLM）中，注意力（Attention）机制和多层感知机（MLP）是构建模型的两个核心组件，它们共同工作以实现深度学习和有效的文本处理。下面分别解释注意力机制的作用以及它与MLP如何协同工作。

#### MLP

MLP的主要体现在以下几个方面：

1. **特征提取**：MLP 是一种前馈神经网络，能够通过多个隐藏层学习输入数据的复杂特征。
   1. LLM中，MLP通常用于处理和转换来自自注意力机制的输出，以提取更高层次的特征表示。
   2. 使得模型能够更好地理解和生成文本。
2. **非线性变换**：MLP通过激活函数**引入非线性**，使得模型能够**学习到复杂的非线性关系**。这对于处理自然语言中的复杂模式和上下文信息至关重要。
3. **增强模型表达能力**：在LLM的架构中，MLP通常位于自注意力层之后，负责将注意力机制的输出进一步处理。通过增加隐藏层的数量，MLP能够捕捉到更抽象的特征表示，从而提升模型的整体性能。
4. **处理不同模态**：在多模态大型语言模型（MLLM）中，MLP也被用于将不同模态（如文本、图像、音频等）的特征进行融合，使得模型能够在处理多种输入时保持一致性和有效性。
5. **计算效率**：在推理过程中，MLP的计算效率对于整体模型的性能至关重要。通过优化MLP的结构和参数，可以提高模型的推理速度和响应时间。

通过特征提取、非线性变换和增强模型的表达能力，帮助模型更好地理解和生成自然语言。

#### Attention

1. **捕捉上下文信息**：
   - 注意力机制使模型能够在序列的任意位置聚焦，捕捉与当前处理的单词或短语最相关的上下文信息
   - 这对于理解语言中的长距离依赖关系至关重要。
2. **权重分配**：
   - 通过为输入序列中的不同部分分配不同的权重，注意力机制允许模型动态地关注输入**序列中对当前预测最有信息量的部分**。
3. **并行处理能力**：
   - 与传统的循环神经网络（RNN）相比，注意力机制可以更容易地实现并行处理，因为它不依赖于序列中先前状态的信息，从而提高了计算效率。
4. **可解释性**：
   - 注意力权重提供了模型决策过程的透明度，使得研究者和开发者可以观察和理解模型在做出预测时关注的信息。

#### MLP与Attention

在Transformer架构中，MLP和注意力机制通常在每个编码器和解码器层中顺序工作，如下所示：

1. **自注意力层**：
   - 首先，模型通过自注意力层处理输入，这一层计算输入序列中每个元素对其他所有元素的注意力权重，并生成加权的上下文表示。
2. **残差连接和层归一化**：
   - 自注意力层的输出通常会通过一个残差连接，然后进行层归一化。这有助于避免在深层网络中出现的梯度消失或爆炸问题。
3. **MLP层**：
   - 接着，模型的输出进入一个前馈网络（通常由两个MLP组成，中间有一个激活函数）。这个MLP层对自注意力层的输出进行进一步的非线性变换，增强模型的表达能力。
4. **再次的残差连接和层归一化**：
   - MLP层的输出同样会通过一个残差连接和层归一化，然后与自注意力层的输出相加，确保信息在层间的流动。
5. **迭代过程**：
   - 在每个编码器或解码器层中，自注意力和MLP的组合被重复使用，每一层都在前一层的基础上进一步提炼和转换信息。

通过这种结构，注意力机制允许模型在处理每个单词时都考虑到整个输入序列的上下文，而MLP则对这些上下文信息进行进一步的加工和抽象，使得模型能够捕捉更复杂的特征和模式。

这种协同工作使得Transformer架构及其衍生模型在处理序列数据时表现出色，特别是在NLP任务中。

### 时间预测

使用了多种方式的时间预测。

定义了基类`BaseExecutionTimePredictor`，使用多种参数支持自定义的时间预测方法配置

#### 基础时间预测

时间预测分为了两个计算：流水线之间的通信开销和张量并行之间的通信开销

#### MLP时间预测

定义了学习类`SklearnExecutionTimePredictor`，该类又有两个子类，分别使用了不同的机器学习方法，需要继承抽象方法`_get_estimator`来实现相应的ml预测方法

1. 线性的多项式回归
   1. 创建了一个包含多步操作的管道
   2. 将输入的特征转换为多项式特征
   3. 在多项式特征空间中找到最佳拟合曲线
2. 随机森林：也是论文中推荐的预测算法

#### 训练过程

其他的互助手段，如加载模型到Cache暂且不表，主要的训练流程为：

1. 如果Cache文件夹中已经由训练过的模型，则从Cache中加载该模型
2. 确定交叉验证的参数

在训练过程中会训练多个模型

1. 训练相关的时间
   1. mlp阶段所需要的运行时间
   2. attention机制需要的KV-cache所需要的运行时间
   3. 通信op所需要的时间
2. 与cpu相关的、在计算以外时间
3. 与注意力层相关的时间
   1. 预填充的时间
   2. 解码时间

### 算力估计

在`vidur/utils/mfu_calculator.py`中实现，实现思路为：

1. 利用传入的replica参数，得到了每个训练实体的相关参数，得到了设备的总性能flops

   1. 每个设备的层数
   2. 每个设备的head数
   3. 每个head的维度数

2. 利用相应的参数计算得到MLP阶段和Attention阶段所需要的计算性能

   1. MLP：MLP通常用于处理和转换来自自注意力机制的输出，以提取更高层次的特征表示。这使得模型能够更好地理解和生成文本

      MLP层在Transformer模型中通常**执行点积**（point-wise）操作，这些操作的计算量主要取决于批处理中的token总数

   2. Attention：负责计算序列中每个元素对其他元素的**注意力权重**，过程依赖于序列中的每个request的上下文长度。

      在Vidur中，由于attention操作在解码阶段主要是内存绑定（memory-bound）的操作，其运行时间主要取决于需要**从KV-Cache中获取的数据总量**，而不是batch中不同request之间上下文长度的具体分配。

      因此，Attention的计算需要遍历所有的request，以准确模拟kernel的运行时间，尤其是在处理不同的KV-Cache和query tokens时，这会导致输入组合的空间非常大

3. 利用当前所需的性能/设备能提供的性能得到当前阶段的利用率

## 训练数据

训练数据在`data/profiling`下，包含了两部分数据：

- compute的数据：运行时（推理）的相关数据
- network的数据：网络分析不依赖于模型，因此可以为所有模型使用相同的网络分析数据。

## 可视化结果

在simulate的过程中，支持两种trace：

- **event_trace**：通常用于记录特定事件的详细信息，包括事件的开始和结束时间、持续时间、线程ID等。
  - 这种追踪方式更关注于细粒度的事件记录，适合于分析系统内部的操作和性能瓶颈
  - 为json格式
- **chrome_trace**：是Vidur导出的模拟结果，采用Chrome的追踪格式。
  - 它可以通过`chrome://tracing/`或`edge://tracing/`进行可视化，主要用于展示模拟过程中的整体性能和事件流。
  - Chrome Trace提供了一个用户友好的界面，便于查看和分析事件的时间线和性能数据
  - 为Chrome Trace格式
