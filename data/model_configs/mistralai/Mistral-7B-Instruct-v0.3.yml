architectures:
  - MistralForCausalLM
attention_dropout: 0.0
bos_token_id: 1
eos_token_id: 2
hidden_act: "silu"
hidden_size: 4096
initializer_range: 0.02
intermediate_size: 14336
model_type: "mistral"
num_attention_heads: 32
num_hidden_layers: 32
num_key_value_heads: 8
rms_norm_eps: 1e-05
sliding_window: null
tie_word_embeddings: false
torch_dtype: "bfloat16"
transformers_version: "4.42.0.dev0"
use_cache: true

num_layers: 32
num_q_heads: 32
num_kv_heads: 8
embedding_dim: 4096
mlp_hidden_dim: 14336
max_position_embeddings: 32768
use_gated_mlp: false
use_bias: false
use_qkv_bias: false
activation: silu
norm: rms_norm
post_attn_norm: false
rope_theta: 1000000.0
rope_scaling: null
vocab_size: 32768
is_neox_style: false
